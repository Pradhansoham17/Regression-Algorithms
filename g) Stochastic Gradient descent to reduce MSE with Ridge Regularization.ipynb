{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x  y\n",
       "0  2  1\n",
       "1  3  2\n",
       "2  1  2\n",
       "3  1  1\n",
       "4  0  1\n",
       "5  5  3\n",
       "6  4  3\n",
       "7  6  7\n",
       "8  5  6\n",
       "9  3  5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('C:\\\\Users\\\\soham\\\\Downloads\\\\LinearRegression.csv')\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21606"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MeanSquaredError(Y_true,Y_pred):\n",
    "    MSE = np.square(np.subtract(Y_true,Y_pred)).mean() \n",
    "    return MSE\n",
    "\n",
    "Y_true = [1,1,2,2,4]\n",
    "Y_pred = [0.6,1.29,1.99,2.69,3.4]\n",
    "\n",
    "MeanSquaredError(Y_true,Y_pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoc_gradient_descent_regularization(X, y_true, lr=0.05, epoch=100, lamda=0.1): #updating m and c till fixed number of iteration/epoch,by default epoch=100\n",
    "    \n",
    "    '''\n",
    "    Gradient Descent for a single feature\n",
    "    '''\n",
    "    \n",
    "    m, c = 3,2 # parameters\n",
    "    logs, mse = [], [] # lists to store learning process\n",
    "    #N = len(X) # number of samples\n",
    "    \n",
    "    for ep in range(epoch):\n",
    "        \n",
    "        logs.append((m, c))\n",
    "        \n",
    "        # lists for plotting purposes\n",
    "        m_list.append(m)\n",
    "        c_list.append(c)\n",
    "        \n",
    "        MSE_complete = MeanSquaredError(y_true, (m_list[-1]*X)+c_list[-1]) \n",
    "        #MSE of complete dataset using latest values of 'm' and 'c' after applying\n",
    "        #stochastic GD on all the points one by one.\n",
    "        print('The Mean Squared Error of epoch ',ep,' is ',MSE_complete)\n",
    "        mse.append(MSE_complete)\n",
    "        mse_list.append(MSE_complete) #for plotting purposes\n",
    "        \n",
    "        for n,i in enumerate(X):\n",
    "            y_pred = (m*i + c)\n",
    "            diff = y_pred - y_true[n] \n",
    "\n",
    "            MSE_single= MeanSquaredError(y_true, y_pred) #for a single point\n",
    "            \n",
    "            m -= lr * ((2 * i * diff) + 2*lamda*m)  # acc to gradient descent eqn, this is m(new) = (m(old) + α * ∂(MSE)/∂m ) , where α is learning rate\n",
    "            # added the regularization factor '2*lamda*m', in the calculation of ∂(MSE)/∂m, no changes in clculation of 'c'\n",
    "            c -= lr * (2 * diff)  # acc to gradient descent eqn, this is c(new) = (c(old) + α * ∂(MSE)/∂c ) , where α is learning rate\n",
    "        \n",
    "    print('The Final Value of m is : ',m)\n",
    "    print('The Final Value of c is : ',c)\n",
    "    print('The Minimum Value of MSE is : ',min(mse))\n",
    "    \n",
    "           \n",
    "    return logs, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoc_gradient_descent_regularization_test(X, y_true, lr=0.05, epoch=100, lamda=0.1): #updating m and c till fixed number of iteration/epoch,by default epoch=100\n",
    "    \n",
    "    '''\n",
    "    Gradient Descent for a single feature\n",
    "    '''\n",
    "    \n",
    "    m, c = 3,2 # parameters\n",
    "    logs, mse = [], [] # lists to store learning process\n",
    "    #N = len(X) # number of samples\n",
    "    \n",
    "    for ep in range(epoch):\n",
    "        \n",
    "        logs.append((m, c))\n",
    "        \n",
    "        # lists for plotting purposes\n",
    "        m_list.append(m)\n",
    "        c_list.append(c)\n",
    "        \n",
    "        MSE_complete = MeanSquaredError(y_true, (m_list[-1]*X)+c_list[-1]) \n",
    "        #MSE of complete dataset using latest values of 'm' and 'c' after applying\n",
    "        #stochastic GD on all the points one by one.\n",
    "        print('The Mean Squared Error of epoch ',ep,' is ',MSE_complete)\n",
    "        mse.append(MSE_complete)\n",
    "        mse_list.append(MSE_complete) #for plotting purposes\n",
    "        \n",
    "        for n,i in enumerate(X):\n",
    "            y_pred = (m*i + c)\n",
    "            diff = y_pred - y_true[n] \n",
    "\n",
    "            MSE_single= MeanSquaredError(y_true, y_pred) #for a single point\n",
    "            \n",
    "            m -= lr * ((2 * i * diff) + 2*lamda*m)  # acc to gradient descent eqn, this is m(new) = (m(old) + α * ∂(MSE)/∂m ) , where α is learning rate\n",
    "            # added the regularization factor '2*lamda*m', in the calculation of ∂(MSE)/∂m, no changes in clculation of 'c'\n",
    "            c -= lr * ((2 * diff) + 2*lamda*c) # acc to gradient descent eqn, this is c(new) = (c(old) + α * ∂(MSE)/∂c ) , where α is learning rate\n",
    "        \n",
    "    print('The Final Value of m is : ',m)\n",
    "    print('The Final Value of c is : ',c)\n",
    "    print('The Minimum Value of MSE is : ',min(mse))\n",
    "    \n",
    "           \n",
    "    return logs, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists for plotting purposes\n",
    "mse_list=[]\n",
    "m_list=[]\n",
    "c_list=[]\n",
    "\n",
    "X=df['x']\n",
    "y=df['y']\n",
    "stoc_gradient_descent_regularization(X, y, lr=0.001, epoch=200) #keep the learning rate low in case of Stochastic GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error of epoch  0  is  79.9\n",
      "The Mean Squared Error of epoch  1  is  47.03028191586601\n",
      "The Mean Squared Error of epoch  2  is  28.020878301684455\n",
      "The Mean Squared Error of epoch  3  is  17.01616173594128\n",
      "The Mean Squared Error of epoch  4  is  10.63688239098411\n",
      "The Mean Squared Error of epoch  5  is  6.932284366701715\n",
      "The Mean Squared Error of epoch  6  is  4.775783572586476\n",
      "The Mean Squared Error of epoch  7  is  3.5164150164996903\n",
      "The Mean Squared Error of epoch  8  is  2.7777722045211704\n",
      "The Mean Squared Error of epoch  9  is  2.3420059741712493\n",
      "The Mean Squared Error of epoch  10  is  2.082882497206221\n",
      "The Mean Squared Error of epoch  11  is  1.92714058414494\n",
      "The Mean Squared Error of epoch  12  is  1.8321760185140405\n",
      "The Mean Squared Error of epoch  13  is  1.7731477878706001\n",
      "The Mean Squared Error of epoch  14  is  1.7355230802505077\n",
      "The Mean Squared Error of epoch  15  is  1.7107630344025666\n",
      "The Mean Squared Error of epoch  16  is  1.6938231376203252\n",
      "The Mean Squared Error of epoch  17  is  1.6817027915812601\n",
      "The Mean Squared Error of epoch  18  is  1.6726020093946317\n",
      "The Mean Squared Error of epoch  19  is  1.6654298529958491\n",
      "The Mean Squared Error of epoch  20  is  1.659516957618294\n",
      "The Mean Squared Error of epoch  21  is  1.6544467034021029\n",
      "The Mean Squared Error of epoch  22  is  1.6499555375897295\n",
      "The Mean Squared Error of epoch  23  is  1.6458737305846949\n",
      "The Mean Squared Error of epoch  24  is  1.642089872912523\n",
      "The Mean Squared Error of epoch  25  is  1.6385293852505458\n",
      "The Mean Squared Error of epoch  26  is  1.6351413543884754\n",
      "The Mean Squared Error of epoch  27  is  1.631890356596891\n",
      "The Mean Squared Error of epoch  28  is  1.628751298363835\n",
      "The Mean Squared Error of epoch  29  is  1.6257061043791503\n",
      "The Mean Squared Error of epoch  30  is  1.6227415521255455\n",
      "The Mean Squared Error of epoch  31  is  1.6198478293992007\n",
      "The Mean Squared Error of epoch  32  is  1.6170175555335944\n",
      "The Mean Squared Error of epoch  33  is  1.6142451055298803\n",
      "The Mean Squared Error of epoch  34  is  1.6115261357884534\n",
      "The Mean Squared Error of epoch  35  is  1.608857246515035\n",
      "The Mean Squared Error of epoch  36  is  1.6062357384250656\n",
      "The Mean Squared Error of epoch  37  is  1.6036594355669007\n",
      "The Mean Squared Error of epoch  38  is  1.6011265551756015\n",
      "The Mean Squared Error of epoch  39  is  1.5986356113977958\n",
      "The Mean Squared Error of epoch  40  is  1.5961853436671563\n",
      "The Mean Squared Error of epoch  41  is  1.5937746631761294\n",
      "The Mean Squared Error of epoch  42  is  1.5914026127260446\n",
      "The Mean Squared Error of epoch  43  is  1.5890683365236906\n",
      "The Mean Squared Error of epoch  44  is  1.586771057405895\n",
      "The Mean Squared Error of epoch  45  is  1.5845100596308928\n",
      "The Mean Squared Error of epoch  46  is  1.582284675853082\n",
      "The Mean Squared Error of epoch  47  is  1.5800942772483109\n",
      "The Mean Squared Error of epoch  48  is  1.5779382660158132\n",
      "The Mean Squared Error of epoch  49  is  1.5758160696753478\n",
      "The Mean Squared Error of epoch  50  is  1.573727136721749\n",
      "The Mean Squared Error of epoch  51  is  1.5716709333067178\n",
      "The Mean Squared Error of epoch  52  is  1.5696469406985099\n",
      "The Mean Squared Error of epoch  53  is  1.5676546533310596\n",
      "The Mean Squared Error of epoch  54  is  1.5656935772999676\n",
      "The Mean Squared Error of epoch  55  is  1.5637632291974355\n",
      "The Mean Squared Error of epoch  56  is  1.5618631352044348\n",
      "The Mean Squared Error of epoch  57  is  1.559992830378208\n",
      "The Mean Squared Error of epoch  58  is  1.5581518580881855\n",
      "The Mean Squared Error of epoch  59  is  1.5563397695647816\n",
      "The Mean Squared Error of epoch  60  is  1.5545561235341074\n",
      "The Mean Squared Error of epoch  61  is  1.5528004859181723\n",
      "The Mean Squared Error of epoch  62  is  1.5510724295850775\n",
      "The Mean Squared Error of epoch  63  is  1.5493715341374477\n",
      "The Mean Squared Error of epoch  64  is  1.5476973857301881\n",
      "The Mean Squared Error of epoch  65  is  1.5460495769107983\n",
      "The Mean Squared Error of epoch  66  is  1.5444277064771086\n",
      "The Mean Squared Error of epoch  67  is  1.5428313793485502\n",
      "The Mean Squared Error of epoch  68  is  1.541260206447984\n",
      "The Mean Squared Error of epoch  69  is  1.5397138045918506\n",
      "The Mean Squared Error of epoch  70  is  1.5381917963869276\n",
      "The Mean Squared Error of epoch  71  is  1.5366938101323908\n",
      "The Mean Squared Error of epoch  72  is  1.5352194797261913\n",
      "The Mean Squared Error of epoch  73  is  1.5337684445749937\n",
      "The Mean Squared Error of epoch  74  is  1.532340349507089\n",
      "The Mean Squared Error of epoch  75  is  1.5309348446878535\n",
      "The Mean Squared Error of epoch  76  is  1.5295515855374022\n",
      "The Mean Squared Error of epoch  77  is  1.5281902326501868\n",
      "The Mean Squared Error of epoch  78  is  1.5268504517163213\n",
      "The Mean Squared Error of epoch  79  is  1.525531913444492\n",
      "The Mean Squared Error of epoch  80  is  1.5242342934863222\n",
      "The Mean Squared Error of epoch  81  is  1.522957272362094\n",
      "The Mean Squared Error of epoch  82  is  1.5217005353877517\n",
      "The Mean Squared Error of epoch  83  is  1.520463772603121\n",
      "The Mean Squared Error of epoch  84  is  1.5192466787012997\n",
      "The Mean Squared Error of epoch  85  is  1.518048952959169\n",
      "The Mean Squared Error of epoch  86  is  1.5168702991689966\n",
      "The Mean Squared Error of epoch  87  is  1.515710425571098\n",
      "The Mean Squared Error of epoch  88  is  1.514569044787529\n",
      "The Mean Squared Error of epoch  89  is  1.5134458737567864\n",
      "The Mean Squared Error of epoch  90  is  1.5123406336695004\n",
      "The Mean Squared Error of epoch  91  is  1.5112530499050874\n",
      "The Mean Squared Error of epoch  92  is  1.5101828519693596\n",
      "The Mean Squared Error of epoch  93  is  1.509129773433066\n",
      "The Mean Squared Error of epoch  94  is  1.5080935518713479\n",
      "The Mean Squared Error of epoch  95  is  1.5070739288041\n",
      "The Mean Squared Error of epoch  96  is  1.5060706496372185\n",
      "The Mean Squared Error of epoch  97  is  1.5050834636047223\n",
      "The Mean Squared Error of epoch  98  is  1.5041121237117345\n",
      "The Mean Squared Error of epoch  99  is  1.5031563866783109\n",
      "The Mean Squared Error of epoch  100  is  1.5022160128840993\n",
      "The Mean Squared Error of epoch  101  is  1.5012907663138297\n",
      "The Mean Squared Error of epoch  102  is  1.5003804145035993\n",
      "The Mean Squared Error of epoch  103  is  1.4994847284879655\n",
      "The Mean Squared Error of epoch  104  is  1.4986034827478212\n",
      "The Mean Squared Error of epoch  105  is  1.4977364551590373\n",
      "The Mean Squared Error of epoch  106  is  1.496883426941875\n",
      "The Mean Squared Error of epoch  107  is  1.496044182611146\n",
      "The Mean Squared Error of epoch  108  is  1.4952185099271056\n",
      "The Mean Squared Error of epoch  109  is  1.4944061998470846\n",
      "The Mean Squared Error of epoch  110  is  1.4936070464778308\n",
      "The Mean Squared Error of epoch  111  is  1.4928208470285582\n",
      "The Mean Squared Error of epoch  112  is  1.4920474017646972\n",
      "The Mean Squared Error of epoch  113  is  1.4912865139623253\n",
      "The Mean Squared Error of epoch  114  is  1.4905379898632758\n",
      "The Mean Squared Error of epoch  115  is  1.4898016386309139\n",
      "The Mean Squared Error of epoch  116  is  1.4890772723065655\n",
      "The Mean Squared Error of epoch  117  is  1.488364705766597\n",
      "The Mean Squared Error of epoch  118  is  1.4876637566801245\n",
      "The Mean Squared Error of epoch  119  is  1.4869742454673582\n",
      "The Mean Squared Error of epoch  120  is  1.486295995258561\n",
      "The Mean Squared Error of epoch  121  is  1.4856288318536142\n",
      "The Mean Squared Error of epoch  122  is  1.4849725836821877\n",
      "The Mean Squared Error of epoch  123  is  1.4843270817644962\n",
      "The Mean Squared Error of epoch  124  is  1.4836921596726416\n",
      "The Mean Squared Error of epoch  125  is  1.4830676534925273\n",
      "The Mean Squared Error of epoch  126  is  1.4824534017863376\n",
      "The Mean Squared Error of epoch  127  is  1.4818492455555727\n",
      "The Mean Squared Error of epoch  128  is  1.481255028204637\n",
      "The Mean Squared Error of epoch  129  is  1.4806705955049606\n",
      "The Mean Squared Error of epoch  130  is  1.4800957955596603\n",
      "The Mean Squared Error of epoch  131  is  1.4795304787687185\n",
      "The Mean Squared Error of epoch  132  is  1.4789744977946853\n",
      "The Mean Squared Error of epoch  133  is  1.478427707528886\n",
      "The Mean Squared Error of epoch  134  is  1.4778899650581299\n",
      "The Mean Squared Error of epoch  135  is  1.4773611296319134\n",
      "The Mean Squared Error of epoch  136  is  1.4768410626301143\n",
      "The Mean Squared Error of epoch  137  is  1.4763296275311597\n",
      "The Mean Squared Error of epoch  138  is  1.475826689880669\n",
      "The Mean Squared Error of epoch  139  is  1.475332117260563\n",
      "The Mean Squared Error of epoch  140  is  1.4748457792586323\n",
      "The Mean Squared Error of epoch  141  is  1.4743675474385534\n",
      "The Mean Squared Error of epoch  142  is  1.4738972953103588\n",
      "The Mean Squared Error of epoch  143  is  1.4734348983013317\n",
      "The Mean Squared Error of epoch  144  is  1.4729802337273488\n",
      "The Mean Squared Error of epoch  145  is  1.472533180764637\n",
      "The Mean Squared Error of epoch  146  is  1.4720936204219546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Squared Error of epoch  147  is  1.4716614355131845\n",
      "The Mean Squared Error of epoch  148  is  1.4712365106303356\n",
      "The Mean Squared Error of epoch  149  is  1.470818732116943\n",
      "The Mean Squared Error of epoch  150  is  1.4704079880418657\n",
      "The Mean Squared Error of epoch  151  is  1.4700041681734728\n",
      "The Mean Squared Error of epoch  152  is  1.4696071639542112\n",
      "The Mean Squared Error of epoch  153  is  1.4692168684755549\n",
      "The Mean Squared Error of epoch  154  is  1.4688331764533253\n",
      "The Mean Squared Error of epoch  155  is  1.4684559842033749\n",
      "The Mean Squared Error of epoch  156  is  1.4680851896176352\n",
      "The Mean Squared Error of epoch  157  is  1.4677206921405235\n",
      "The Mean Squared Error of epoch  158  is  1.4673623927456925\n",
      "The Mean Squared Error of epoch  159  is  1.4670101939131341\n",
      "The Mean Squared Error of epoch  160  is  1.466663999606618\n",
      "The Mean Squared Error of epoch  161  is  1.4663237152514654\n",
      "The Mean Squared Error of epoch  162  is  1.4659892477126573\n",
      "The Mean Squared Error of epoch  163  is  1.4656605052732652\n",
      "The Mean Squared Error of epoch  164  is  1.4653373976132016\n",
      "The Mean Squared Error of epoch  165  is  1.4650198357882893\n",
      "The Mean Squared Error of epoch  166  is  1.4647077322096407\n",
      "The Mean Squared Error of epoch  167  is  1.4644010006233426\n",
      "The Mean Squared Error of epoch  168  is  1.4640995560904435\n",
      "The Mean Squared Error of epoch  169  is  1.4638033149672391\n",
      "The Mean Squared Error of epoch  170  is  1.4635121948858532\n",
      "The Mean Squared Error of epoch  171  is  1.4632261147351024\n",
      "The Mean Squared Error of epoch  172  is  1.4629449946416475\n",
      "The Mean Squared Error of epoch  173  is  1.4626687559514298\n",
      "The Mean Squared Error of epoch  174  is  1.462397321211375\n",
      "The Mean Squared Error of epoch  175  is  1.4621306141513766\n",
      "The Mean Squared Error of epoch  176  is  1.4618685596665428\n",
      "The Mean Squared Error of epoch  177  is  1.4616110837997127\n",
      "The Mean Squared Error of epoch  178  is  1.4613581137242249\n",
      "The Mean Squared Error of epoch  179  is  1.46110957772695\n",
      "The Mean Squared Error of epoch  180  is  1.4608654051915704\n",
      "The Mean Squared Error of epoch  181  is  1.4606255265821133\n",
      "The Mean Squared Error of epoch  182  is  1.4603898734267229\n",
      "The Mean Squared Error of epoch  183  is  1.460158378301683\n",
      "The Mean Squared Error of epoch  184  is  1.4599309748156675\n",
      "The Mean Squared Error of epoch  185  is  1.459707597594231\n",
      "The Mean Squared Error of epoch  186  is  1.4594881822645307\n",
      "The Mean Squared Error of epoch  187  is  1.459272665440274\n",
      "The Mean Squared Error of epoch  188  is  1.459060984706888\n",
      "The Mean Squared Error of epoch  189  is  1.4588530786069176\n",
      "The Mean Squared Error of epoch  190  is  1.4586488866256293\n",
      "The Mean Squared Error of epoch  191  is  1.4584483491768414\n",
      "The Mean Squared Error of epoch  192  is  1.4582514075889557\n",
      "The Mean Squared Error of epoch  193  is  1.4580580040912063\n",
      "The Mean Squared Error of epoch  194  is  1.4578680818001009\n",
      "The Mean Squared Error of epoch  195  is  1.4576815847060778\n",
      "The Mean Squared Error of epoch  196  is  1.4574984576603538\n",
      "The Mean Squared Error of epoch  197  is  1.4573186463619654\n",
      "The Mean Squared Error of epoch  198  is  1.457142097345014\n",
      "The Mean Squared Error of epoch  199  is  1.4569687579660884\n",
      "The Final Value of m is :  0.8663758522777981\n",
      "The Final Value of c is :  0.5979209274077304\n",
      "The Minimum Value of MSE is :  1.4569687579660884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(3, 2),\n",
       "  (2.440593930321635, 1.8541286455147492),\n",
       "  (2.0162858633988447, 1.7415194275756802),\n",
       "  (1.6945528792002529, 1.6541794711338482),\n",
       "  (1.4507007229427986, 1.5860397766439749),\n",
       "  (1.2659793551278735, 1.5324921218502583),\n",
       "  (1.1261521103633205, 1.4900374366341402),\n",
       "  (1.0204092761183268, 1.4560188180756972),\n",
       "  (0.9405431855712345, 1.4284188118556025),\n",
       "  (0.880321875099704, 1.4057054903502157),\n",
       "  (0.8350135096195687, 1.3867155814944625),\n",
       "  (0.8010252842167962, 1.3705657298665157),\n",
       "  (0.7756292463118949, 1.356585118244903),\n",
       "  (0.7567541155846784, 1.3442643079266394),\n",
       "  (0.7428272152392436, 1.3332163937640762),\n",
       "  (0.7326544522330948, 1.3231475096260332),\n",
       "  (0.7253291876470295, 1.3138344335285759),\n",
       "  (0.7201630430065489, 1.3051075834633086),\n",
       "  (0.7166333623189095, 1.296838106320322),\n",
       "  (0.7143433206034453, 1.2889280746508636),\n",
       "  (0.7129916347587433, 1.2813030431769918),\n",
       "  (0.7123495653737165, 1.2739063970300102),\n",
       "  (0.7122434544684988, 1.2666950604279854),\n",
       "  (0.7125414666031599, 1.2596362383190531),\n",
       "  (0.7131435215552419, 1.2527049423437937),\n",
       "  (0.7139736503187819, 1.2458821123220971),\n",
       "  (0.7149741911034494, 1.2391531899149528),\n",
       "  (0.7161013824245963, 1.2325070356174967),\n",
       "  (0.7173220169883595, 1.225935106439568),\n",
       "  (0.7186109010262309, 1.2194308315233227),\n",
       "  (0.7199489251981389, 1.21298913805222),\n",
       "  (0.7213215998524518, 1.2066060912745487),\n",
       "  (0.7227179428668472, 1.2002786211728456),\n",
       "  (0.724129635199765, 1.1940043149225774),\n",
       "  (0.7255503797114213, 1.1877812593038888),\n",
       "  (0.7269754143250511, 1.18160792104217),\n",
       "  (0.7284011423769055, 1.1754830559475766),\n",
       "  (0.7298248519463137, 1.1694056399212918),\n",
       "  (0.731244502747276, 1.1633748165649906),\n",
       "  (0.7326585643187424, 1.1573898573969557),\n",
       "  (0.7340658931653858, 1.151450131640311),\n",
       "  (0.735465639473031, 1.1455550832792853),\n",
       "  (0.7368571762797688, 1.139704213634042),\n",
       "  (0.7382400456974104, 1.1338970681257197),\n",
       "  (0.7396139180790584, 1.1281332262230868),\n",
       "  (0.7409785610165075, 1.122412293804989),\n",
       "  (0.7423338158013167, 1.116733897357111),\n",
       "  (0.7436795795529536, 1.1110976795615422),\n",
       "  (0.7450157916498714, 1.1055032959439126),\n",
       "  (0.7463424234277505, 1.0999504123235633),\n",
       "  (0.7476594703584488, 1.094438702873474),\n",
       "  (0.7489669461125223, 1.0889678486432108),\n",
       "  (0.750264878051909, 1.083537536433465),\n",
       "  (0.7515533038085147, 1.078147457937577),\n",
       "  (0.7528322686873031, 1.0727973090858156),\n",
       "  (0.7541018236954158, 1.0674867895436295),\n",
       "  (0.7553620240466233, 1.0622156023268379),\n",
       "  (0.7566129280266836, 1.0569834535056444),\n",
       "  (0.7578545961327231, 1.0517900519761165),\n",
       "  (0.7590870904206783, 1.0466351092829196),\n",
       "  (0.7603104740107046, 1.0415183394809981),\n",
       "  (0.7615248107125234, 1.0364394590268575),\n",
       "  (0.7627301647418288, 1.031398186692341),\n",
       "  (0.7639266005058308, 1.0263942434955249),\n",
       "  (0.765114182441284, 1.0214273526446307),\n",
       "  (0.7662929748923626, 1.0164972394918501),\n",
       "  (0.7674630420187842, 1.0116036314947232),\n",
       "  (0.7686244477268951, 1.0067462581832782),\n",
       "  (0.7697772556181817, 1.0019248511315735),\n",
       "  (0.7709215289510101, 0.9971391439326043),\n",
       "  (0.7720573306124013, 0.9923888721757925),\n",
       "  (0.7731847230974217, 0.9876737734264653),\n",
       "  (0.7743037684943473, 0.9829935872068654),\n",
       "  (0.77541452847421, 0.978348054978352),\n",
       "  (0.7765170642836604, 0.9737369201245293),\n",
       "  (0.7776114367403462, 0.9691599279351062),\n",
       "  (0.7786977062301937, 0.9646168255903339),\n",
       "  (0.7797759327061282, 0.9601073621459071),\n",
       "  (0.7808461756878815, 0.9556312885182429),\n",
       "  (0.7819084942626183, 0.9511883574700688),\n",
       "  (0.7829629470861802, 0.9467783235962701),\n",
       "  (0.7840095923847916, 0.9424009433099592),\n",
       "  (0.7850484879571107, 0.9380559748287338),\n",
       "  (0.7860796911765385, 0.9337431781611065),\n",
       "  (0.7871032589937149, 0.929462315093083),\n",
       "  (0.7881192479391559, 0.9252131491748798),\n",
       "  (0.789127714125988, 0.9209954457077693),\n",
       "  (0.790128713252754, 0.9168089717310438),\n",
       "  (0.7911223006062665, 0.9126534960090933),\n",
       "  (0.7921085310644913, 0.908528789018592),\n",
       "  (0.7930874590994509, 0.9044346229357895),\n",
       "  (0.7940591387801346, 0.9003707716239032),\n",
       "  (0.7950236237754119, 0.8963370106206089),\n",
       "  (0.7959809673569389, 0.8923331171256296),\n",
       "  (0.7969312224020594, 0.8883588699884194),\n",
       "  (0.7978744413966922, 0.8844140496959392),\n",
       "  (0.798810676438206, 0.8804984383605273),\n",
       "  (0.799739979238278, 0.8766118197078597),\n",
       "  (0.8006624011257353, 0.8727539790650005),\n",
       "  (0.8015779930493803, 0.8689247033485427),\n",
       "  (0.8024868055807959, 0.8651237810528359),\n",
       "  (0.803388888917133, 0.8613510022383036),\n",
       "  (0.8042842928838776, 0.8576061585198456),\n",
       "  (0.8051730669375999, 0.8538890430553268),\n",
       "  (0.8060552601686832, 0.8501994505341531),\n",
       "  (0.806930921304033, 0.8465371771659295),\n",
       "  (0.8078000987097671, 0.8429020206692042),\n",
       "  (0.8086628403938858, 0.8392937802602962),\n",
       "  (0.8095191940089225, 0.8357122566422032),\n",
       "  (0.810369206854575, 0.8321572519935952),\n",
       "  (0.8112129258803179, 0.8286285699578873),\n",
       "  (0.812050397687995, 0.8251260156323935),\n",
       "  (0.8128816685343934, 0.8216493955575629),\n",
       "  (0.8137067843337974, 0.8181985177062925),\n",
       "  (0.8145257906605254, 0.814773191473322),\n",
       "  (0.8153387327514467, 0.811373227664705),\n",
       "  (0.8161456555084794, 0.8079984384873594),\n",
       "  (0.8169466035010711, 0.8046486375386954),\n",
       "  (0.8177416209686603, 0.8013236397963188),\n",
       "  (0.8185307518231199, 0.798023261607812),\n",
       "  (0.8193140396511825, 0.7947473206805901),\n",
       "  (0.8200915277168486, 0.7914956360718322),\n",
       "  (0.8208632589637744, 0.7882680281784876),\n",
       "  (0.8216292760176461, 0.7850643187273545),\n",
       "  (0.8223896211885325, 0.7818843307652348),\n",
       "  (0.8231443364732224, 0.7787278886491589),\n",
       "  (0.8238934635575444, 0.7755948180366851),\n",
       "  (0.8246370438186696, 0.7724849458762696),\n",
       "  (0.8253751183273962, 0.7693981003977087),\n",
       "  (0.826107727850419, 0.7663341111026505),\n",
       "  (0.8268349128525807, 0.7632928087551786),\n",
       "  (0.8275567134991064, 0.7602740253724635),\n",
       "  (0.8282731696578228, 0.7572775942154849),\n",
       "  (0.8289843209013601, 0.7543033497798222),\n",
       "  (0.8296902065093372, 0.7513511277865121),\n",
       "  (0.830390865470532, 0.7484207651729763),\n",
       "  (0.8310863364850344, 0.7455121000840137),\n",
       "  (0.8317766579663844, 0.7426249718628612),\n",
       "  (0.8324618680436933, 0.7397592210423195),\n",
       "  (0.8331420045637498, 0.7369146893359455),\n",
       "  (0.8338171050931105, 0.7340912196293095),\n",
       "  (0.8344872069201748, 0.731288655971317),\n",
       "  (0.8351523470572448, 0.7285068435655949),\n",
       "  (0.8358125622425695, 0.7257456287619417),\n",
       "  (0.836467888942373, 0.7230048590478407),\n",
       "  (0.8371183633528706, 0.7202843830400362),\n",
       "  (0.8377640214022665, 0.7175840504761718),\n",
       "  (0.8384048987527385, 0.7149037122064912),\n",
       "  (0.8390410308024085, 0.7122432201855997),\n",
       "  (0.8396724526872968, 0.7096024274642877),\n",
       "  (0.8402991992832638, 0.7069811881814135),\n",
       "  (0.8409213052079347, 0.7043793575558466),\n",
       "  (0.8415388048226133, 0.7017967918784718),\n",
       "  (0.842151732234179, 0.6992333485042507),\n",
       "  (0.842760121296971, 0.6966888858443436),\n",
       "  (0.8433640056146577, 0.6941632633582887),\n",
       "  (0.8439634185420931, 0.6916563415462397),\n",
       "  (0.8445583931871598, 0.6891679819412596),\n",
       "  (0.8451489624125958, 0.6866980471016744),\n",
       "  (0.8457351588378119, 0.6842464006034804),\n",
       "  (0.8463170148406919, 0.6818129070328091),\n",
       "  (0.8468945625593812, 0.6793974319784486),\n",
       "  (0.8474678338940624, 0.6769998420244185),\n",
       "  (0.8480368605087171, 0.674620004742602),\n",
       "  (0.8486016738328741, 0.6722577886854297),\n",
       "  (0.8491623050633461, 0.6699130633786208),\n",
       "  (0.8497187851659531, 0.6675856993139759),\n",
       "  (0.8502711448772323, 0.6652755679422239),\n",
       "  (0.8508194147061352, 0.6629825416659213),\n",
       "  (0.8513636249357138, 0.660706493832405),\n",
       "  (0.8519038056247931, 0.6584472987267964),\n",
       "  (0.8524399866096306, 0.6562048315650586),\n",
       "  (0.8529721975055651, 0.6539789684871027),\n",
       "  (0.8535004677086516, 0.6517695865499474),\n",
       "  (0.8540248263972863, 0.6495765637209284),\n",
       "  (0.8545453025338163, 0.6473997788709576),\n",
       "  (0.8550619248661413, 0.645239111767833),\n",
       "  (0.8555747219293005, 0.643094443069598),\n",
       "  (0.8560837220470483, 0.640965654317949),\n",
       "  (0.8565889533334197, 0.638852627931694),\n",
       "  (0.8570904436942819, 0.6367552472002556),\n",
       "  (0.8575882208288774, 0.6346733962772272),\n",
       "  (0.8580823122313523, 0.6326069601739727),\n",
       "  (0.8585727451922754, 0.6305558247532763),\n",
       "  (0.8590595468001464, 0.6285198767230373),\n",
       "  (0.8595427439428907, 0.6264990036300133),\n",
       "  (0.8600223633093453, 0.6244930938536083),\n",
       "  (0.8604984313907333, 0.6225020365997084),\n",
       "  (0.8609709744821263, 0.620525721894561),\n",
       "  (0.8614400186838972, 0.6185640405787018),\n",
       "  (0.8619055899031619, 0.6166168843009247),\n",
       "  (0.86236771385521, 0.6146841455122971),\n",
       "  (0.862826416064925, 0.6127657174602201),\n",
       "  (0.8632817218681951, 0.6108614941825314),\n",
       "  (0.863733656413311, 0.6089713705016538),\n",
       "  (0.8641822446623568, 0.6070952420187844),\n",
       "  (0.8646275113925873, 0.6052330051081292),\n",
       "  (0.8650694811977968, 0.6033845569111789),\n",
       "  (0.8655081784896782, 0.6015497953310285),\n",
       "  (0.8659436274991705, 0.5997286190267369)],\n",
       " [79.9,\n",
       "  47.03028191586601,\n",
       "  28.020878301684455,\n",
       "  17.01616173594128,\n",
       "  10.63688239098411,\n",
       "  6.932284366701715,\n",
       "  4.775783572586476,\n",
       "  3.5164150164996903,\n",
       "  2.7777722045211704,\n",
       "  2.3420059741712493,\n",
       "  2.082882497206221,\n",
       "  1.92714058414494,\n",
       "  1.8321760185140405,\n",
       "  1.7731477878706001,\n",
       "  1.7355230802505077,\n",
       "  1.7107630344025666,\n",
       "  1.6938231376203252,\n",
       "  1.6817027915812601,\n",
       "  1.6726020093946317,\n",
       "  1.6654298529958491,\n",
       "  1.659516957618294,\n",
       "  1.6544467034021029,\n",
       "  1.6499555375897295,\n",
       "  1.6458737305846949,\n",
       "  1.642089872912523,\n",
       "  1.6385293852505458,\n",
       "  1.6351413543884754,\n",
       "  1.631890356596891,\n",
       "  1.628751298363835,\n",
       "  1.6257061043791503,\n",
       "  1.6227415521255455,\n",
       "  1.6198478293992007,\n",
       "  1.6170175555335944,\n",
       "  1.6142451055298803,\n",
       "  1.6115261357884534,\n",
       "  1.608857246515035,\n",
       "  1.6062357384250656,\n",
       "  1.6036594355669007,\n",
       "  1.6011265551756015,\n",
       "  1.5986356113977958,\n",
       "  1.5961853436671563,\n",
       "  1.5937746631761294,\n",
       "  1.5914026127260446,\n",
       "  1.5890683365236906,\n",
       "  1.586771057405895,\n",
       "  1.5845100596308928,\n",
       "  1.582284675853082,\n",
       "  1.5800942772483109,\n",
       "  1.5779382660158132,\n",
       "  1.5758160696753478,\n",
       "  1.573727136721749,\n",
       "  1.5716709333067178,\n",
       "  1.5696469406985099,\n",
       "  1.5676546533310596,\n",
       "  1.5656935772999676,\n",
       "  1.5637632291974355,\n",
       "  1.5618631352044348,\n",
       "  1.559992830378208,\n",
       "  1.5581518580881855,\n",
       "  1.5563397695647816,\n",
       "  1.5545561235341074,\n",
       "  1.5528004859181723,\n",
       "  1.5510724295850775,\n",
       "  1.5493715341374477,\n",
       "  1.5476973857301881,\n",
       "  1.5460495769107983,\n",
       "  1.5444277064771086,\n",
       "  1.5428313793485502,\n",
       "  1.541260206447984,\n",
       "  1.5397138045918506,\n",
       "  1.5381917963869276,\n",
       "  1.5366938101323908,\n",
       "  1.5352194797261913,\n",
       "  1.5337684445749937,\n",
       "  1.532340349507089,\n",
       "  1.5309348446878535,\n",
       "  1.5295515855374022,\n",
       "  1.5281902326501868,\n",
       "  1.5268504517163213,\n",
       "  1.525531913444492,\n",
       "  1.5242342934863222,\n",
       "  1.522957272362094,\n",
       "  1.5217005353877517,\n",
       "  1.520463772603121,\n",
       "  1.5192466787012997,\n",
       "  1.518048952959169,\n",
       "  1.5168702991689966,\n",
       "  1.515710425571098,\n",
       "  1.514569044787529,\n",
       "  1.5134458737567864,\n",
       "  1.5123406336695004,\n",
       "  1.5112530499050874,\n",
       "  1.5101828519693596,\n",
       "  1.509129773433066,\n",
       "  1.5080935518713479,\n",
       "  1.5070739288041,\n",
       "  1.5060706496372185,\n",
       "  1.5050834636047223,\n",
       "  1.5041121237117345,\n",
       "  1.5031563866783109,\n",
       "  1.5022160128840993,\n",
       "  1.5012907663138297,\n",
       "  1.5003804145035993,\n",
       "  1.4994847284879655,\n",
       "  1.4986034827478212,\n",
       "  1.4977364551590373,\n",
       "  1.496883426941875,\n",
       "  1.496044182611146,\n",
       "  1.4952185099271056,\n",
       "  1.4944061998470846,\n",
       "  1.4936070464778308,\n",
       "  1.4928208470285582,\n",
       "  1.4920474017646972,\n",
       "  1.4912865139623253,\n",
       "  1.4905379898632758,\n",
       "  1.4898016386309139,\n",
       "  1.4890772723065655,\n",
       "  1.488364705766597,\n",
       "  1.4876637566801245,\n",
       "  1.4869742454673582,\n",
       "  1.486295995258561,\n",
       "  1.4856288318536142,\n",
       "  1.4849725836821877,\n",
       "  1.4843270817644962,\n",
       "  1.4836921596726416,\n",
       "  1.4830676534925273,\n",
       "  1.4824534017863376,\n",
       "  1.4818492455555727,\n",
       "  1.481255028204637,\n",
       "  1.4806705955049606,\n",
       "  1.4800957955596603,\n",
       "  1.4795304787687185,\n",
       "  1.4789744977946853,\n",
       "  1.478427707528886,\n",
       "  1.4778899650581299,\n",
       "  1.4773611296319134,\n",
       "  1.4768410626301143,\n",
       "  1.4763296275311597,\n",
       "  1.475826689880669,\n",
       "  1.475332117260563,\n",
       "  1.4748457792586323,\n",
       "  1.4743675474385534,\n",
       "  1.4738972953103588,\n",
       "  1.4734348983013317,\n",
       "  1.4729802337273488,\n",
       "  1.472533180764637,\n",
       "  1.4720936204219546,\n",
       "  1.4716614355131845,\n",
       "  1.4712365106303356,\n",
       "  1.470818732116943,\n",
       "  1.4704079880418657,\n",
       "  1.4700041681734728,\n",
       "  1.4696071639542112,\n",
       "  1.4692168684755549,\n",
       "  1.4688331764533253,\n",
       "  1.4684559842033749,\n",
       "  1.4680851896176352,\n",
       "  1.4677206921405235,\n",
       "  1.4673623927456925,\n",
       "  1.4670101939131341,\n",
       "  1.466663999606618,\n",
       "  1.4663237152514654,\n",
       "  1.4659892477126573,\n",
       "  1.4656605052732652,\n",
       "  1.4653373976132016,\n",
       "  1.4650198357882893,\n",
       "  1.4647077322096407,\n",
       "  1.4644010006233426,\n",
       "  1.4640995560904435,\n",
       "  1.4638033149672391,\n",
       "  1.4635121948858532,\n",
       "  1.4632261147351024,\n",
       "  1.4629449946416475,\n",
       "  1.4626687559514298,\n",
       "  1.462397321211375,\n",
       "  1.4621306141513766,\n",
       "  1.4618685596665428,\n",
       "  1.4616110837997127,\n",
       "  1.4613581137242249,\n",
       "  1.46110957772695,\n",
       "  1.4608654051915704,\n",
       "  1.4606255265821133,\n",
       "  1.4603898734267229,\n",
       "  1.460158378301683,\n",
       "  1.4599309748156675,\n",
       "  1.459707597594231,\n",
       "  1.4594881822645307,\n",
       "  1.459272665440274,\n",
       "  1.459060984706888,\n",
       "  1.4588530786069176,\n",
       "  1.4586488866256293,\n",
       "  1.4584483491768414,\n",
       "  1.4582514075889557,\n",
       "  1.4580580040912063,\n",
       "  1.4578680818001009,\n",
       "  1.4576815847060778,\n",
       "  1.4574984576603538,\n",
       "  1.4573186463619654,\n",
       "  1.457142097345014,\n",
       "  1.4569687579660884])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lists for plotting purposes\n",
    "mse_list=[]\n",
    "m_list=[]\n",
    "c_list=[]\n",
    "\n",
    "X=df['x']\n",
    "y=df['y']\n",
    "stoc_gradient_descent_regularization_test(X, y, lr=0.001, epoch=200) #keep the learning rate low in case of Stochastic GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using sklearn SGDRegressor:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [3],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [5],\n",
       "       [4],\n",
       "       [6],\n",
       "       [5],\n",
       "       [3]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df['x']\n",
    "y=df['y']\n",
    "np.array(X).reshape((10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.1, average=False, early_stopping=False, epsilon=0.1,\n",
       "             eta0=0.001, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='constant', loss='squared_loss', max_iter=100,\n",
       "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(max_iter=100, alpha=0.1, learning_rate='constant', eta0=0.001 )\n",
    "reg.fit(np.array(X).reshape(10,1), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91919702])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24989906])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.457420531283298"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MeanSquaredError(y,reg.predict(np.array(X).reshape(10,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
